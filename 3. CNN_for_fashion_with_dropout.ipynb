{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jYysdyb-CaWM"},"source":["\n","\n","\n","![GettingStarted](https://mooc-styleguide.s3.amazonaws.com/MOOC-Styles/Active+Learning+Headers/Links/ALH_GettingStarted.png)\n","#[Step 1] **READ**: Simple Fashion Goods Recognition with Neural Network\n","# **Objective**\n","#### CNN model for image classification\n","\n","We will implement a model to classify the images of retail products (predict and label the images in 10 classes such as dress, coat, or shirt)."]},{"cell_type":"markdown","metadata":{"id":"FbVhjPpzn6BM"},"source":["#[Step 2] **RUN**: Setting Up the Colab Environment\n","\n","**Importing Packages**\n","\n","Python packages enable different functions, providing easy ways of manipulating data and building models. As a first step, we 'import' packages to set up our environment in a way that allows us to take advantage of different capabilities.\n","\n","Import the necessary libraries and print the version of TensorFlow API:"]},{"cell_type":"code","metadata":{"id":"dzLKpmZICaWN"},"source":["# TensorFlow and tf.keras\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.models import load_model\n","import os\n","from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","# Helper libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yR0EdgrLCaWR"},"source":["\n","**Loading Dataset**\n","<br>\n","Next we need to load the Fashion MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [pixels][width][height].\n","\n","In this case, since our dataset is in grey scale, the pixel dimension is set to 1."]},{"cell_type":"markdown","metadata":{"id":"DLdCchMdCaWQ"},"source":["We will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. The following are some examples of the images (each image is 28 x 28 pixels):\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n","         alt=\"Fashion MNIST sprite\"  width=\"600\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","Import and load the data from TensorFow (the data is already split into test and training sets):"]},{"cell_type":"code","metadata":{"id":"7MqDQO0KCaWS"},"source":["fashion_mnist = keras.datasets.fashion_mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ldl-hbFx4Fp"},"source":["# train_images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9FDsUlxCaWW"},"source":["**Data Overview**\n","<br>\n","So let's see what kind of pictures and classes are in this fashion dataset\n","The *images* are defined as pixel values ranging between 0 and 255 (each image with 28x28=784 values). These values correspond to a shade of gray:\n","\n","<table>\n","  <tr><td>\n","    <img src=\"http://what-when-how.com/wp-content/uploads/2012/07/tmp26dc25_thumb2.png\"\n","         alt=\"Shades of grey\" width=\"400\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure 2.</b> 256 (not 50) shades of grey.<br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","The *labels* are integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n","\n","<table>\n","  <tr>\n","    <th>Label</th>\n","    <th>Class</th>\n","  </tr>\n","  <tr>\n","    <td>0</td>\n","    <td>T-shirt/top</td>\n","  </tr>\n","  <tr>\n","    <td>1</td>\n","    <td>Trouser</td>\n","  </tr>\n","    <tr>\n","    <td>2</td>\n","    <td>Pullover</td>\n","  </tr>\n","    <tr>\n","    <td>3</td>\n","    <td>Dress</td>\n","  </tr>\n","    <tr>\n","    <td>4</td>\n","    <td>Coat</td>\n","  </tr>\n","    <tr>\n","    <td>5</td>\n","    <td>Sandal</td>\n","  </tr>\n","    <tr>\n","    <td>6</td>\n","    <td>Shirt</td>\n","  </tr>\n","    <tr>\n","    <td>7</td>\n","    <td>Sneaker</td>\n","  </tr>\n","    <tr>\n","    <td>8</td>\n","    <td>Bag</td>\n","  </tr>\n","    <tr>\n","    <td>9</td>\n","    <td>Ankle boot</td>\n","  </tr>\n","</table>\n","\n","Assign *class names* to the labels (we will use the class names later when plotting the images):"]},{"cell_type":"code","metadata":{"id":"IjnLH5S2CaWx"},"source":["#Assign the class names that are correspond to each label\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Brm0b_KACaWX"},"source":["**Explore the data**\n","\n","Explore the format of the **training** dataset (60,000 images with each image is represented as 28 x 28 pixels):"]},{"cell_type":"code","metadata":{"id":"zW5k_xz1CaWX"},"source":["# View the shape of the training images\n","train_images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YSlYxFuRCaWk"},"source":["Explore the labels of the **training** dataset (each label is an integer between 0 and 9):"]},{"cell_type":"code","metadata":{"id":"XKnCTHz4CaWg"},"source":["# View the labels from the train set. The labels has digits vary from 0 to 9 to correspond with each clothes item\n","# See the image above\n","train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TMPI88iZpO2T"},"source":["Explore the format of the **test** dataset (10,000 images with each image is represented as 28 x 28 pixels):"]},{"cell_type":"code","metadata":{"id":"2KFnYlcwCaWl"},"source":["# View the shape of the testing images\n","test_images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rd0A0Iu0CaWq"},"source":["Explore the labels of the **test** dataset (each label is an integer between 0 and 9):"]},{"cell_type":"code","metadata":{"id":"iJmPr5-ACaWn"},"source":["# View the labels from the test set.\n","test_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ES6uQoLKCaWr"},"source":["**Preprocess the data**\n","\n","See how a sample image looks like (28 by 28 pixels, each pixel having a value between 0 and 255):"]},{"cell_type":"code","metadata":{"id":"m4VEw8Ud9Quh"},"source":["# We are going to display the first image in the dataset\n","plt.figure()\n","plt.imshow(train_images[0],cmap=plt.cm.binary)\n","plt.colorbar() #display the color bar to see the grey scale\n","plt.grid(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wz7l27Lz9S1P"},"source":["**Normalization**\n","<br>\n","Then we normalize the pixel values to the range 0 and 1 for both the **training** and **test** datasets (this is how the neural network model expects the input):"]},{"cell_type":"code","metadata":{"id":"bW5WzIPlCaWv"},"source":["train_images = train_images / 255.0\n","\n","test_images = test_images / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji1e-3ghBCuA"},"source":["Confirm that the pixel values are now between 0 and 1:\n","\n","Quick note: Check the color bar and spot the difference"]},{"cell_type":"code","metadata":{"id":"Womwp5CjA_2-"},"source":["# Display normalized image\n","plt.figure()\n","plt.imshow(train_images[0],cmap=plt.cm.binary)\n","plt.colorbar()\n","plt.grid(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ee638AlnCaWz"},"source":["**Examine some of the images in the dataset**\n","<br>\n","Display the first 25 images from the *training set* with the class name below each image and verify that the data looks good:"]},{"cell_type":"code","metadata":{"id":"oZTImqg_CaW1"},"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_images[i], cmap=plt.cm.binary)\n","    plt.xlabel(class_names[train_labels[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59veuiEZCaW4"},"source":["## Build the model\n","\n","After checking the data, we are going to build the neural network which requires\n","1. setting up the layers of the model\n","2. compiling the model"]},{"cell_type":"markdown","metadata":{"id":"Gxg1XGm0eOBy"},"source":["#[Step 3] **RUN**: Setting Up the Neural Network Architecture\n","\n","**Design the neural network architecture**\n","\n","The basic building block of a neural network is the *layer*. Layers extract representations from the data. Create three layers:\n","\n","\n","\n","1. First, we are going to create the model framework.\n","2. Next we will add the flatten later which we can input an image that is 28 * 28. This layer will flatten the image to a vector so we can pass it in to the neural network. Each pixel of the image will become an individual input that feeds in the next hidden layer\n","3. The next layer is a fully connected layer. Because the neurals are all close to each other, we cal it the \"dense\" layer . Each neuron on this layer are fully connected to the last layer. It takes the input from last layer, aggregate them and run them through a \"relu\" function. \"Relu\" is an activation function which will transform the aggregated result in each neuron\n","4. The next layer is our output layer. The output layer has 10 neurons because we have 10 different digits in this classification problems. We feed the results from last layer to a **softmax activation function** to output probability-like predictions for each class. The digits with the highest probability is what the algorithm thought the digit to be\n","5. The model is trained using sparse categorical crossentropy loss function and the ADAM gradient descent algorithm.\n"]},{"cell_type":"code","metadata":{"id":"f_oalx7gDEJv"},"source":["#reshape the input and output\n","trainX = train_images.reshape((train_images.shape[0], 28, 28, 1))\n","testX = test_images.reshape((test_images.shape[0], 28, 28, 1))\n","trainY = train_labels\n","testY = test_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAFBgmxAyxlM"},"source":["**Setup The Layers**"]},{"cell_type":"code","metadata":{"id":"WPZuEpTQjMLl"},"source":["def baseline_model():\n","    # Create a model\n","    model = Sequential()\n","\n","    # add the convolutional layer\n","    # filters, size of filters,padding,activation_function,input_shape\n","    model.add(Conv2D(256,(3,3),padding=\"SAME\",activation=\"relu\",input_shape=(28,28,1)))\n","    # pooling layer\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","\n","    model.add(Conv2D(128,(3,3),padding=\"SAME\",activation=\"relu\"))\n","    # pooling layer\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","    # place a dropout layer (prevent overfitting)\n","    # 0.5 drop out rate is recommended, half input nodes will be dropped at each update\n","    model.add(Dropout(0.5))\n","\n","    # model.add(Conv2D(128,(3,3),padding=\"SAME\",activation=\"relu\"))\n","    # # pooling layer\n","    # model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","    # place a dropout layer (prevent overfitting)\n","    # 0.5 drop out rate is recommended, half input nodes will be dropped at each update\n","    # model.add(Dropout(0.5))\n","\n","    model.add(Flatten())\n","    # Add different layers to your model\n","    #model.add(Flatten(input_shape=(28, 28))) # DO NOT CHANGE ANYTHING IN THIS LAYER\n","    model.add(Dense(128, activation='relu'))\n","\n","    # 0.5 drop out rate is recommended, half input nodes will be dropped at each update\n","    model.add(Dropout(0.5))\n","\n","    #output layer\n","    model.add(Dense(10, activation='softmax')) # DO NOT CHANGE ANYTHING IN THIS LAYER\n","    # Compile model\n","    model.compile(loss='sparse_categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKF6uW-BCaW-"},"source":["---\n","![APPLY](https://mooc-styleguide.s3.amazonaws.com/MOOC-Styles/Active+Learning+Headers/Links/ALH_Apply.png)\n","#[Step 4] **APPLY**: Convolutional Neural Network for Fashion Item Recognition\n","\n","With our environment set up and data loaded, we can now train, test, and evaluate our model to recognize the fashion items. To do this, we will use the train set and test set we created to train our model.\n","Running the example, the accuracy on the training and validation test is printed each epoch and at the end of the classification error rate is printed.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4Dnt8DM-2tTj"},"source":["**Create the model**"]},{"cell_type":"code","metadata":{"id":"xvwvpA64CaW_"},"source":["# Call the function we defined earlier to create the model\n","model = baseline_model()\n","\n","#check the parameters in the model\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ileWoy72hbw"},"source":["**Train the model**"]},{"cell_type":"code","metadata":{"id":"ITxDqulf1jP_"},"source":["# Train the model using the picture stored in train_images and the corresponding labels stored in train_labels\n","\n","# shuffle=true will shuffle the indexes of the instances in order to allocate different sets of instances\n","# in validation dataset each time cross-validation runs\n","\n","# validation split=0.1 means a validation dataset of size of 10% is created from the training dataset for cross validation\n","\n","\n","history = model.fit(trainX, trainY, batch_size =128, epochs=20, shuffle=True, validation_split=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDM42PEDgdC-"},"source":["Accuracy Model"]},{"cell_type":"code","metadata":{"id":"yP1LGIVfkjtw"},"source":["print(history.history.keys())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_p7dIu3jPgt"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zPssWgvf_Iq"},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8U4jppwghn-"},"source":["Loss Model"]},{"cell_type":"code","metadata":{"id":"KJQGpoAqgmLx"},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oEw4bZgGCaXB"},"source":["**Display the model performance on the test set**\n","\n","<br>\n","X_test include the new fashion images the model has never seen\n","<br>\n","y_test include the correct labels of the test set"]},{"cell_type":"markdown","metadata":{"id":"C9aVlzrx11ve"},"source":["## check test performance"]},{"cell_type":"code","metadata":{"id":"VflXLEeECaXC"},"source":["# Display the test set accuracy and loss\n","test_loss, test_acc = model.evaluate(testX, testY)\n","# Print out the accuracy\n","print('Test accuracy:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsoS7CPDCaXH"},"source":["**Make predictions**\n","\n","Store the predictions for all images in the test dataset:"]},{"cell_type":"code","metadata":{"id":"Gl91RPhdCaXI"},"source":["# Make predictions using the model\n","predictions = model.predict(testX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9Kk1voUCaXJ"},"source":["Display the probabilities in the prediction of the first image. Each number in the numpy array is the probability correspond to the 10 classes"]},{"cell_type":"code","metadata":{"id":"3DmJEUinCaXK"},"source":["# See the results (shown as probability) from the first image\n","predictions[0] #222 for a false prediction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hw1hgeSCaXN"},"source":["Display the label that is prediced with the highest probability:"]},{"cell_type":"code","metadata":{"id":"qsqenuPnCaXO"},"source":["# From all the probabilities the model gave us, we will select the highest one to be our final results\n","# argmax() will help us do that\n","np.argmax(predictions[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E51yS7iCCaXO"},"source":["Confirm whether the prediction is correct by checking the actual label of the first image:"]},{"cell_type":"code","metadata":{"id":"Sd7Pgsu6CaXP"},"source":["# Display the actual label for the first test image\n","class_names[test_labels[0]], class_names[np.argmax(predictions[0])]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wa1w5SSMpISR"},"source":["#[Step 5] **DISCUSS**: What to do next?\n","\n","You will test different neural network structures, train and evaluate the model, and report the **Test accuracy** each time. Test accuracy is reported in the **\"Evaluate the model\"** step. Follow instructions from the assignment."]},{"cell_type":"markdown","metadata":{"id":"N8ninG_VmvbN"},"source":["---\n","\n","# Copyright 2018 The TensorFlow Authors."]},{"cell_type":"code","metadata":{"cellView":"form","id":"XzEU3kdsl41k"},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"vmQidcwJmAX2"},"source":["#@title MIT License\n","#\n","# Copyright (c) 2017 François Chollet\n","#\n","# Permission is hereby granted, free of charge, to any person obtaining a\n","# copy of this software and associated documentation files (the \"Software\"),\n","# to deal in the Software without restriction, including without limitation\n","# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n","# and/or sell copies of the Software, and to permit persons to whom the\n","# Software is furnished to do so, subject to the following conditions:\n","#\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n","# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n","# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n","# DEALINGS IN THE SOFTWARE."],"execution_count":null,"outputs":[]}]}